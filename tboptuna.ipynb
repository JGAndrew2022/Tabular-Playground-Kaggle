{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport dateutil.easter as easter # Used to get Easter date each year, which is a significant holiday in Nordic Countries\n\n#Load all of the datasets that will be used\ntrain = pd.read_csv('../input/tabular-playground-series-jan-2022/train.csv')\ntest = pd.read_csv('../input/tabular-playground-series-jan-2022/test.csv')\ngdp_df = pd.read_csv('../input/consumer-price-index-20152019-nordic-countries/Best_CPI.csv')\ngdp_pc_df = pd.read_csv('../input/gdp-per-capita-finland-norway-sweden-201519/GDP_per_capita_2015_to_2019_Finland_Norway_Sweden.csv')\nmacro_df = pd.read_csv('../input/macroeconomic-composite-finland-norway-sweden/macro_economic_idx.csv')\n\n# Function to process the date column, including getting holidays\ndef dateProcess1(df, gdp_df):\n    # Make a bunch of columns for the dates\n    day_mon_list = []\n    mon_list = []\n    year_list = []\n\n    for k in range(len(df['date'])):\n        splt = df.iloc[k]['date'].split('-')\n        day_mon_list.append(int(splt[2]))\n        mon_list.append(int(splt[1]))\n        year_list.append(int(splt[0]) - 2015)\n    df['day_of_month'] = day_mon_list\n    df['month'] = mon_list\n    df['year'] = year_list\n    \n    # Add GDP Data\n    gdp_list = []\n    gdp_pc_list = []\n    for i in range(len(df['year'])):\n        if(df.iloc[i]['country'] == 'Finland'):\n            gdp_list.append(gdp_df.iloc[(3*df.iloc[i]['year'])]['GDP'])\n            gdp_pc_list.append(gdp_pc_df.iloc[df.iloc[i]['year']]['Finland'])\n        elif(df.iloc[i]['country'] == 'Norway'):\n            gdp_list.append(gdp_df.iloc[(3*df.iloc[i]['year']) + 1]['GDP'])\n            gdp_pc_list.append(gdp_pc_df.iloc[df.iloc[i]['year']]['Norway'])\n        elif(df.iloc[i]['country'] == 'Sweden'):\n            gdp_list.append(gdp_df.iloc[(3*df.iloc[i]['year']) + 2]['GDP'])\n            gdp_pc_list.append(gdp_pc_df.iloc[df.iloc[i]['year']]['Sweden'])\n    df['gdp_list'] = gdp_list\n    df['gdp_per_capita'] = gdp_pc_list\n   \n    # Add macro data\n    macro_list = []\n    for j in range(len(df['year'])):\n        if(df.iloc[j]['country'] == 'Finland'): \n            if(df.iloc[j]['product'] == 'Kaggle Hat'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year']]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Mug'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 5]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Sticker'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 10]['macro_comp'])\n        elif(df.iloc[j]['country'] == 'Sweden'): \n            if(df.iloc[j]['product'] == 'Kaggle Hat'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 15]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Mug'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 20]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Sticker'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 25]['macro_comp'])\n        elif(df.iloc[j]['country'] == 'Norway'): \n            if(df.iloc[j]['product'] == 'Kaggle Hat'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 30]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Mug'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 35]['macro_comp'])\n            elif(df.iloc[j]['product'] == 'Kaggle Sticker'): \n                macro_list.append(macro_df.iloc[df.iloc[j]['year'] + 40]['macro_comp'])\n    df['macro_data'] = macro_list      \n    \n    \n    df['date'] = pd.to_datetime(df['date'])\n    df['weekend'] = df.date.dt.weekday >= 5 # Saturday and Sunday\n    df['friday'] = df.date.dt.weekday == 4 # Friday\n    df['day_of_year'] = df.date.dt.dayofyear\n    \n    # Christmas\n    xmas_date = df.date.dt.year.apply(lambda year: pd.Timestamp(str(year)+'-12-25'))\n    df['xmas_adjust'] = (df.date - xmas_date).dt.days.clip(lower=-20,upper=16).astype(float)\n          \n    # Easter\n    easter_date = df.date.apply(lambda date: pd.Timestamp(easter.easter(date.year)))\n    df['easter_adj']= (df.date - easter_date).dt.days.clip(lower =-3,upper = 60).astype(float)\n    df.loc[df['easter_adj'].isin(range(12, 39)), 'easter_adj'] = 12 \n    \n    # Black Friday\n    black_fri_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-27')),\n                                         2016: pd.Timestamp(('2016-11-25')),\n                                         2017: pd.Timestamp(('2017-11-24')),\n                                         2018: pd.Timestamp(('2018-11-23')),\n                                         2019: pd.Timestamp(('2019-11-29'))})\n    df['days_from_black_friday'] = (df.date - black_fri_date).dt.days.clip(-5, 5)\n    \n    # Last Wednesday of June\n    wed_june_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-06-24')),\n                                         2016: pd.Timestamp(('2016-06-29')),\n                                         2017: pd.Timestamp(('2017-06-28')),\n                                         2018: pd.Timestamp(('2018-06-27')),\n                                         2019: pd.Timestamp(('2019-06-26'))})\n    df['days_from_wed_jun'] = (df.date - wed_june_date).dt.days.clip(-5, 5)\n    \n    #First Sunday of November (second Sunday is Father's Day)\n    sun_nov_date = df.date.dt.year.map({2015: pd.Timestamp(('2015-11-1')),\n                                         2016: pd.Timestamp(('2016-11-6')),\n                                         2017: pd.Timestamp(('2017-11-5')),\n                                         2018: pd.Timestamp(('2018-11-4')),\n                                         2019: pd.Timestamp(('2019-11-3'))})\n    df['days_from_sun_nov'] = (df.date - sun_nov_date).dt.days.clip(-1, 9)\n    \n    print(df['date'])\n    df.drop(columns=['date'],inplace=True)\n\ndateProcess1(train, gdp_df)\ndateProcess1(test, gdp_df)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T02:43:00.535363Z","iopub.execute_input":"2022-01-31T02:43:00.535855Z","iopub.status.idle":"2022-01-31T02:43:45.838371Z","shell.execute_reply.started":"2022-01-31T02:43:00.535773Z","shell.execute_reply":"2022-01-31T02:43:45.837675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OneHotEncoder\nimport category_encoders as ce\n\n# Nearly all of our data is categorical, and we do not know a clear correlation between categories and num_sold, so we will hot encode using scikit-learn's OneHotEnocder\ndef dataProcess(x):\n    one_hot = ce.OneHotEncoder(cols = ['country'])\n    x = one_hot.fit_transform(x)\n\n    one_hot1 = ce.OneHotEncoder(cols = ['store']) # Creating a new hot encoder for each column may not be the most efficient, feel free to optimize this\n    x = one_hot1.fit_transform(x)\n\n    one_hot2 = ce.OneHotEncoder(cols = ['product'])\n    x = one_hot2.fit_transform(x)\n    return x","metadata":{"execution":{"iopub.status.busy":"2022-01-31T02:43:45.839774Z","iopub.execute_input":"2022-01-31T02:43:45.839961Z","iopub.status.idle":"2022-01-31T02:43:46.823069Z","shell.execute_reply.started":"2022-01-31T02:43:45.839937Z","shell.execute_reply":"2022-01-31T02:43:46.822264Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nfeatures = ['country', 'store', 'product', 'day_of_month', 'month', 'year', 'day_of_year', 'weekend', 'friday', 'xmas_adjust', 'easter_adj', 'days_from_black_friday', 'days_from_wed_jun', 'days_from_sun_nov', 'gdp_list', 'macro_data']\nlabels = ['num_sold']\nx_train = train[features]\ny_train = train[labels]\ny_train = np.ravel(y_train) # Scikit-learn didn't like my y-column unless I used this .ravel() method from numpy\nx_test = test[features]\n\n\nx_train = dataProcess(x_train)\nx_test = dataProcess(x_test)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T02:43:46.824374Z","iopub.execute_input":"2022-01-31T02:43:46.82469Z","iopub.status.idle":"2022-01-31T02:43:46.993482Z","shell.execute_reply.started":"2022-01-31T02:43:46.82466Z","shell.execute_reply":"2022-01-31T02:43:46.992964Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.preprocessing import OrdinalEncoder\n\n# Canvert True/False values to numerical data\nobject_cols = ['weekend', 'friday']\n\nordinal_encoder = OrdinalEncoder()\nx_train[object_cols] = ordinal_encoder.fit_transform(x_train[object_cols])\nx_train[object_cols] = ordinal_encoder.transform(x_train[object_cols])\n\nx_test[object_cols] = ordinal_encoder.fit_transform(x_test[object_cols])\nx_test[object_cols] = ordinal_encoder.transform(x_test[object_cols])","metadata":{"execution":{"iopub.status.busy":"2022-01-31T02:43:46.994879Z","iopub.execute_input":"2022-01-31T02:43:46.99542Z","iopub.status.idle":"2022-01-31T02:43:47.012661Z","shell.execute_reply.started":"2022-01-31T02:43:46.995393Z","shell.execute_reply":"2022-01-31T02:43:47.011938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split","metadata":{"execution":{"iopub.status.busy":"2022-01-31T02:43:47.013644Z","iopub.execute_input":"2022-01-31T02:43:47.013846Z","iopub.status.idle":"2022-01-31T02:43:47.056985Z","shell.execute_reply.started":"2022-01-31T02:43:47.013807Z","shell.execute_reply":"2022-01-31T02:43:47.056336Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import RandomForestRegressor\n# X-treme gradient boost\nfrom xgboost import XGBRegressor\n\n# CatBoost\nimport catboost\nfrom catboost import CatBoostRegressor\n\n# Light Gradient Boosting Machine\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import StackingRegressor","metadata":{"execution":{"iopub.status.busy":"2022-01-31T02:43:47.057989Z","iopub.execute_input":"2022-01-31T02:43:47.058206Z","iopub.status.idle":"2022-01-31T02:43:48.392019Z","shell.execute_reply.started":"2022-01-31T02:43:47.058182Z","shell.execute_reply":"2022-01-31T02:43:48.391255Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# This is the way that the competition will grade our predictions\ndef SMAPE(y_true, y_pred):\n    diff = np.abs(y_true - y_pred) / (y_true + np.abs(y_pred)) * 200\n    return diff.mean()\n\nimport optuna\n\n\ndef objective(trial):\n    \n    rf_n_estimators_grad = trial.suggest_int(\"rf_n_estimators\", 10, 5000)\n    \n    grad_boost = RandomForestRegressor(n_estimators=rf_n_estimators_grad)\n    \n    x_train_train, x_train_test, y_train_train, y_train_test = train_test_split(x_train, y_train, test_size=0.2) # split the data so we can get an idea of our model's performance\n    \n    # Step 3: Scoring method:\n    grad_boost.fit(x_train_train, y_train_train)\n    y_pred = grad_boost.predict(x_train_test)\n    for z in range(len(y_pred)):\n        y_pred[z] = round(float(y_pred[z]))\n    smape_train = SMAPE(y_train_test, y_pred)\n    return smape_train\n\n# Step 4: Running it\n# previous best is 4.522593002459165\nstudy = optuna.create_study(direction=\"minimize\")\nstudy.optimize(objective, n_trials=100)","metadata":{"execution":{"iopub.status.busy":"2022-01-31T02:43:48.393337Z","iopub.execute_input":"2022-01-31T02:43:48.393825Z","iopub.status.idle":"2022-01-31T03:21:14.006666Z","shell.execute_reply.started":"2022-01-31T02:43:48.393786Z","shell.execute_reply":"2022-01-31T03:21:14.005507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"rf_n_estimators_xg = trial.suggest_int(\"rf_n_estimators_xg\", 10, 5000)\n    rf_max_depth_xg = trial.suggest_int(\"rf_max_depth_xg\", 2, 16)\n    rf_learning_rate_xg = trial.suggest_float(\"rf_learning_rate_xg\", 0.001, 0.5)\n    \nxg_boost = XGBRegressor(max_depth=rf_max_depth_xg, n_estimators=rf_n_estimators_xg, learning_rate=rf_learning_rate_xg)\n    \n    rf_learning_rate_cat = trial.suggest_float(\"rf_alpha\", 0.01, 0.5)\n    rf_max_depth_cat = trial.suggest_int(\"rf_learning_rate_init\", 2, 16)\n    rf_n_estimators_cat = trial.suggest_int(\"rf_n_estimators\", 10, 5000)\n    \n    cat_boost = CatBoostRegressor(learning_rate=rf_learning_rate_cat, max_depth = rf_max_depth_cat, n_estimators = rf_n_estimators_cat)\n    \n    rf_learning_rate_lg = trial.suggest_float(\"rf_alpha_lg\", 0.01, 0.5)\n    rf_max_depth_lg = trial.suggest_int(\"rf_learning_rate_init_lg\", 2, 16)\n    rf_n_estimators_lg = trial.suggest_int(\"rf_n_estimators_lg\", 10, 5000)\n    \n    lg_boost = LGBMRegressor(learning_rate=rf_learning_rate_lg, max_depth = rf_max_depth_lg, n_estimators = rf_n_estimators_lg)\n    \n    learning_rate_fin = trial.suggest_float(\"rf_alpha\", 0.01, 0.5)\n    max_depth_fin = trial.suggest_int(\"rf_learning_rate_init\", 2, 16)\n    n_estimators_fin = trial.suggest_int(\"rf_n_estimators\", 10, 5000)\n    \n    final_boost = CatBoostRegressor(learning_rate=learning_rate_fin, max_depth = max_depth_fin, n_estimators = n_estimators_fin)\n    \n    regressor_obj = StackingRegressor([('grad', grad_boost), ('xg', xg_boost), ('cat', cat_boost), ('lg', lg_boost)], final_estimator = final_boost)\n    ","metadata":{}}]}